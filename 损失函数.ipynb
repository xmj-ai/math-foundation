{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": "# 交叉熵函数是什么？与最大似然函数的联系和区别？\n## 二分类\n$P(y\u003d1|x;\\theta)\u003dh_{\\theta}(x)$ # 输入x，模型参数为$\\theta$的情况下，输出结果为1\n的概率\n\n$P(y\u003d0|x;\\theta)\u003d1-h_{\\theta}(x)$ # 输入x，模型参数为$\\theta$的情况下，输出结果为0\n的概率\n\n$\\hat{y}\u003dh_{\\theta}(x)$表示模型预测值\n\n此时交叉熵为：\n\n$P(y|x;\\theta)\u003d(h_{\\theta}(x))^{y}*(1-h_{\\theta}(x))^{(1-y)}$\n\n熵的本质是香农信息量$\\log \\frac{1}{p}$的期望。\n\n现有关于样本集的2个概率分布p和q，其中p为真实分布，q非真实分布。按照真实分布p来衡量识别一个样本的所需要的编码长度的期望(即平均编码长度)为：\n\n$H(p)\u003d\\sum p(x)\\log \\frac{1}{p(x)}\u003d-\\sum p(x)\\log p(x)$\n\n如果使用错误分布q来表示来自真实分布p的平均编码长度，则应该是：\n\n$H(p,q)\u003d\\sum p(x)\\log \\frac{1}{q(x)}\u003d-\\sum p(x)\\log q(x)$\n\n相对熵公式如下：\n$D_{KL}{p||q}\u003d\\sum p(x)\\log \\frac{p(x)}{q(x)}\u003d\\sum p(x)\\log p(x)-\\sum p(x)\\log q(x)\u003d-H(p)+Hp(q)$,\n\n交叉熵:\n\n$H_{p}(q)\u003d\\sum_{i\u003d1}^{n} p(x) \\log q(x)$\n\n似然函数：$L(\\theta)\u003d\\prod_{i\u003d1}^{m}P(y^i|x^i;\\theta)\u003d\\prod_{i\u003d1}^{m} \\left[(h_{\\theta})^{y^i}*(1-h_{\\theta})^{1-y^i}\\right]$\n\n对数似然函数：$\\log L(\\theta)\u003d\\log \\prod_{i\u003d1}^{m}P(y^i|x^i;\\theta)\u003d\\log \\prod_{i\u003d1}^{m} \\left[(h_{\\theta})^{y^i}*(1-h_{\\theta})^{1-y^i}\\right]\u003d\\sum_{i\u003d1}^{m}   \\left[ {y^i}\\log(h_{\\theta})+{(1-y^i)}\\log (1-h_{\\theta})\\right]$\n\n显然似然函数越大，交叉熵越小，区别$P(y^i|x^i;\\theta)$也可以是任意分布，似然函数可以对任意分布使用，线性回归用的就是正太分布。\n\n\n# 线性回归损失函数和似然函数（MLE）\n## 似然函数\n$y^{(i)}\u003d\\theta^{T}x^{(i)}+\\epsilon^(i)$\n\n一维正态概率密度函数\n$f(x)\u003d\\frac{1}{\\sqrt {2\\pi \\sigma}} \\exp^{-({\\frac{(x-\\mu)^2}{2\\sigma^2}})}$\n\n标准正态分布\n当$\\mu\u003d0,\\sigma\u003d1$时，正态分布就成为标准正态分布\n$f(x)\u003d\\frac{1}{\\sqrt {2\\pi }\\sigma} e^{(-{\\frac{x^2}{2\\sigma^2})}}$\n所以：\n\n$p(\\epsilon^{(i)})\u003d\\frac{1}{\\sqrt {2\\pi }\\sigma} e^{(-{\\frac{{\\epsilon^{(i)}}^2}{2\\sigma^2})}}$\n\n$p(y^{(i)}|x^{(i)};\\theta)\u003d\\frac{1}{\\sqrt {2\\pi }\\sigma} e^{(-{\\frac{(y^{(i)}-\\theta^T x^{(i)})^2}{2\\sigma^2})}}$\n\n$L(\\theta)\u003d\\prod_{i\u003d1}^{m}p(y^{(i)}|x^{(i)}; \\theta)\u003d\\prod_{i\u003d1}^{m}\\frac{1}{\\sqrt {2\\pi }\\sigma} e^{(-{\\frac{(y^{(i)}-\\theta^T x^{(i)})^2}{2\\sigma^2})}}$\n$l(\\theta)\u003dlog \\prod_{i\u003d1}^{m}\\frac{1}{\\sqrt {2\\pi }\\sigma} e^{(-{\\frac{(y^{(i)}-\\theta^T x^{(i)})^2}{2\\sigma^2})}}\u003d\\sum_{i\u003d1}^{m} log \\frac{1}{\\sqrt {2\\pi }\\sigma} e^{(-{\\frac{(y^{(i)}-\\theta^T x^{(i)})^2}{2\\sigma^2})}}$\n$l(\\theta)\u003dm\\log \\frac{1}{\\sqrt {2\\pi }\\sigma}-\\frac{1}{\\sigma^2}\\frac{1}{2}\\sum_{i\u003d1}^{m}(y^{(i)}-\\theta^T x^{(i)})^2$\n为了求导设置$\\frac{1}{\\sigma^2\u003d1}$\n所以：$J(\\theta)\u003d\\frac{1}{2}\\sum_{i\u003d1}^{m}(y^{(i)}-\\theta^T x^{(i)})^2\u003d\\frac{1}{2}\\sum_{i\u003d1}^{m}(h_\\theta(x^{(i)})-\\theta^T x^{(i)})^2$\n \n目标代价函数\n$J(\\theta)\u003d \\frac{1}{2}\\sum_{i\u003d1}^m[(h_{\\theta}(x^{(i)})-y^{(i)})]^2 \u003d\\frac{1}{2} (X\\theta-y)^T*(X\\theta-y)$\n\n## 梯度\n$ \\bigtriangledown_{\\theta}J(\\theta) \u003d\\bigtriangledown_{\\theta}\\frac{1}{2} (X\\theta-y)^T*(X\\theta-y)\u003d\\frac{1}{2} (\\theta^TX^TX\\theta-\\theta^TX^Ty-y^TX\\theta+y^Ty)$\n$ \u003d\\frac{1}{2} (2X^TX\\theta-X^Ty-(y^TX)^T)$\n$ \u003d\\frac{1}{2} (2X^TX\\theta-X^Ty-X^Ty)$\n$ \u003d(X^TX\\theta-X^Ty)\\overset{求驻点}{\\rightarrow}0$\n\n最小二乘下的参数最优解\n$X^TX\\theta-X^Ty\u003d0$\n\n$X^TX\\theta\u003dX^Ty \\Rightarrow \\theta\u003d(X^TX)^{-1}X^Ty$\n\n简单记忆法\n\n$X\\theta\u003dy \\Rightarrow X^TX\\theta\u003dX^Ty \\Rightarrow \\theta\u003d(X^TX)^{-1}X^Ty $\n\n\n\n\n"
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}